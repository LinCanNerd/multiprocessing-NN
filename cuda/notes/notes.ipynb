{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tintando/multiprocessing-NN\n",
    "- Compare performance to pytorch\n",
    "- Parameters sorted by priority:\n",
    "    - Dataset\n",
    "    - Layers (array of sizes)\n",
    "    - Batch size\n",
    "    - Loss function\n",
    "    - Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile\n",
    "\n",
    "shortcut: ctlr + shift + b (run build task)\n",
    "\n",
    "In /cuda/\n",
    "```shell\n",
    "mkdir build\n",
    "cd build\n",
    "cmake ..\n",
    "make\n",
    "```\n",
    "\n",
    "run:\n",
    "./bin/main\n",
    "Remember to edit CMakeLists.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rundown\n",
    "1. Set up the environment for CUDA programming in C++.\n",
    "2. Load the dataset into memory.\n",
    "    1. Define the data structure to hold the dataset.\n",
    "    2. Read the dataset from a file or generate synthetic data.\n",
    "    3. Preprocess the data if necessary (e.g., normalization, feature scaling).\n",
    "3. Split the dataset into training and testing sets.\n",
    "    1. Determine the ratio of training to testing data.\n",
    "    2. Randomly shuffle the dataset.\n",
    "    3. Assign the first portion to the training set and the remaining portion to the testing set.\n",
    "4. Initialize the MLP model.\n",
    "    1. Define the architecture of the MLP (number of layers, number of neurons per layer).\n",
    "    2. Initialize the weights and biases of the MLP.\n",
    "5. Implement the forward pass of the MLP.\n",
    "    1. Compute the weighted sum of inputs and apply the activation function for each layer.\n",
    "    2. Pass the output of each layer as input to the next layer.\n",
    "6. Implement the backward pass (backpropagation) of the MLP.\n",
    "    1. Compute the gradients of the loss function with respect to the weights and biases.\n",
    "    2. Update the weights and biases using gradient descent or other optimization algorithms.\n",
    "7. Train the MLP using the training set.\n",
    "    1. Iterate over the training set.\n",
    "    2. Perform forward pass and backward pass for each input.\n",
    "    3. Update the weights and biases based on the computed gradients.\n",
    "8. Evaluate the performance of the trained MLP using the testing set.\n",
    "    1. Perform forward pass for each input in the testing set.\n",
    "    2. Compare the predicted outputs with the ground truth labels.\n",
    "    3. Calculate evaluation metrics such as accuracy, precision, recall, and F1 score.\n",
    "9. Perform inference using the trained MLP.\n",
    "    1. Load new input data.\n",
    "    2. Perform forward pass to obtain predictions.\n",
    "    3. Interpret the predictions based on the problem domain.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "## Global memory coalescence"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
